from transformers import AutoTokenizer, AutoModelForCausalLM

from util.classes import Generation, ConfigSpec
from generate_text import load_file
from tqdm import tqdm
from typing import List
import json
import sys


sys_prompt = "You are a assistant model to help me to judge if the given text have a watermark and is generated by machine, and please output your judgement 'True' or 'False' without any other content after it."
model_name = "/data2/huggingface-mirror/dataroot/models/meta-llama/Meta-Llama-3-8B-Instruct"
def generate_imperceptibility(config: ConfigSpec, watermark_list: str):
    
    input_path1 = "/data1/lzs/MarkText/generations.json"
    generations1 = Generation.fromfile(input_path1)
    model = AutoModelForCausalLM.from_pretrained(model_name, device_map = "auto")
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    tokenizer.pad_token = tokenizer.eos_token
    percent_list = []
    for watermark_name in watermark_list:
        print(f"watermark_name = {watermark_name}")
        input_path = f"{config.output_path}/watermark/{watermark_name}/generations_{watermark_name}.json"
        generations2 = Generation.fromfile(input_path)
        idx = 0
        num = 0
        for generation in tqdm(generations1,desc="Processing ----------"):
            generation2 = generations2[idx]
            idx += 1

            text2 = generation2.response
            
            input = f"Text: {text2} \n {sys_prompt} \n The result is:"
            le = len(input)
            inputs = tokenizer(
                text=input,
                return_tensors="pt",
                max_length = 2800,
                truncation = True,
            )
            input_ids = inputs["input_ids"]
            output_ids = model.generate(
                input_ids = input_ids.to(model.device),
            )
            output_text = tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0]
            output_text = output_text[le:]
            print(f"output_text ={output_text}")
            if "True" in output_text:
                num += 1
        
        percent = num / len(generations1)
        print(f"False Percent = {percent}")
        percent_list.append(percent)
    
    print(f"All percent = {percent_list}")


if __name__ == "__main__":
    
    config_file = "/home/lsz/MarkText/config.yml"
    
    config = load_file(config_file=config_file)
    
    watermark_list = ["Lexical"]
    generate_imperceptibility(config=config, watermark_list=watermark_list)